{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miladbahrami/ChatGPT-API/blob/main/L1_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3aIxNC4nxAO",
        "outputId": "72925d3f-6d90-4db3-9a09-f9cab8e5eace"
      },
      "id": "i3aIxNC4nxAO",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630",
      "metadata": {
        "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630"
      },
      "source": [
        "# L1 Language Models, the Chat Format and Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f",
      "metadata": {
        "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f"
      },
      "source": [
        "## Setup\n",
        "#### Load the API key and relevant Python libaries.\n",
        "In this course, we've provided some code that loads the OpenAI API key for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "19cd4e96",
      "metadata": {
        "height": 132,
        "id": "19cd4e96"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c",
      "metadata": {
        "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c"
      },
      "source": [
        "#### helper function\n",
        "This may look familiar if you took the earlier course \"ChatGPT Prompt Engineering for Developers\" Course"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1ed96988",
      "metadata": {
        "height": 149,
        "id": "1ed96988"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe10a390-2461-447d-bf8b-8498db404c44",
      "metadata": {
        "id": "fe10a390-2461-447d-bf8b-8498db404c44"
      },
      "source": [
        "## Prompt the model and get a completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e1cc57b2",
      "metadata": {
        "height": 30,
        "id": "e1cc57b2"
      },
      "outputs": [],
      "source": [
        "response = get_completion(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "76774108",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76774108",
        "outputId": "399aa2da-33b9-4655-d2bf-ac9d843b0a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63",
      "metadata": {
        "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63"
      },
      "source": [
        "## Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cc2d9e40",
      "metadata": {
        "height": 64,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc2d9e40",
        "outputId": "973dd4d7-aa8a-4e9e-df7b-10dd86c6e7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reversed letters of \"lollipop\" are \"pillipol\".\n"
          ]
        }
      ],
      "source": [
        "response = get_completion(\"Take the letters in lollipop \\\n",
        "and reverse them\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f",
      "metadata": {
        "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f"
      },
      "source": [
        "\"lollipop\" in reverse should be \"popillol\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "37cab84f",
      "metadata": {
        "height": 47,
        "id": "37cab84f"
      },
      "outputs": [],
      "source": [
        "response = get_completion(\"\"\"Take the letters in \\\n",
        "l-o-l-l-i-p-o-p and reverse them\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1577c561",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1577c561",
        "outputId": "857294b4-37c6-4455-90ab-2475147a7763"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'p-o-p-i-l-l-o-l'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd",
      "metadata": {
        "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd"
      },
      "source": [
        "## Helper function (chat format)\n",
        "Here's the helper function we'll use in this course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8f89efad",
      "metadata": {
        "height": 200,
        "id": "8f89efad"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"gpt-3.5-turbo\",\n",
        "                                 temperature=0,\n",
        "                                 max_tokens=500):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b28c3424",
      "metadata": {
        "height": 183,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b28c3424",
        "outputId": "7a9d452a-222a-4443-d2f0-1c3fd3284fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, once there was a happy carrot,\n",
            "With greens so bright, he'd never falter.\n",
            "Beneath the ground, he grew so tall,\n",
            "He'd greet the sun, and never stall.\n",
            "\n",
            "With joy he soaked up rain and dew,\n",
            "His orange hue grew deeper, too.\n",
            "He'd wiggle his roots, and sway to and fro,\n",
            "In the garden, where he loved to grow.\n",
            "\n",
            "From seed to sprout, his journey was crisp,\n",
            "He laughed and danced, with a playful twist.\n",
            "With every sunrise, his happiness grew,\n",
            "This little carrot, so sunny and true.\n",
            "\n",
            "So let us celebrate this veggie delight,\n",
            "A carrot so happy, it brings pure delight.\n",
            "With a jolly crunch and a vivid hue,\n",
            "Oh, happy carrot, we're grateful for you!\n"
          ]
        }
      ],
      "source": [
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who\\\n",
        " responds in the style of Dr Seuss.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a very short poem\\\n",
        " about a happy carrot\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "56c6978d",
      "metadata": {
        "height": 183,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56c6978d",
        "outputId": "5adef685-e2fb-4d57-cd12-b3de23b5af61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, there was a plump, cheerful carrot named Charlie who lived in a vibrant vegetable garden.\n"
          ]
        }
      ],
      "source": [
        "# length\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':'All your responses must be \\\n",
        "one sentence long.'},\n",
        "{'role':'user',\n",
        " 'content':'write me a story about a happy carrot'},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature =1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "14fd6331",
      "metadata": {
        "height": 217,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14fd6331",
        "outputId": "da6738f0-a7a1-4992-9cc0-3f20c2546881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, there was a happy carrot named Larry who lived in a garden so merry.\n"
          ]
        }
      ],
      "source": [
        "# combined\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who \\\n",
        "responds in the style of Dr Seuss. \\\n",
        "All your responses must be one sentence long.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages,\n",
        "                                        temperature =1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "89a70c79",
      "metadata": {
        "height": 370,
        "id": "89a70c79"
      },
      "outputs": [],
      "source": [
        "def get_completion_and_token_count(messages,\n",
        "                                   model=\"gpt-3.5-turbo\",\n",
        "                                   temperature=0,\n",
        "                                   max_tokens=500):\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message[\"content\"]\n",
        "\n",
        "    token_dict = {\n",
        "'prompt_tokens':response['usage']['prompt_tokens'],\n",
        "'completion_tokens':response['usage']['completion_tokens'],\n",
        "'total_tokens':response['usage']['total_tokens'],\n",
        "    }\n",
        "\n",
        "    return content, token_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a64cf3c6",
      "metadata": {
        "height": 166,
        "id": "a64cf3c6"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who responds\\\n",
        " in the style of Dr Seuss.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a very short poem \\\n",
        " about a happy carrot\"\"\"},\n",
        "]\n",
        "response, token_dict = get_completion_and_token_count(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cfd8fbd4",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfd8fbd4",
        "outputId": "de2d8cd2-580b-4cc7-bc08-fc766617d9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, the happy carrot, so bright and orange,\n",
            "Grown in the garden, a joyful forage.\n",
            "With a smile so wide, from top to bottom,\n",
            "It brings happiness, oh how it blossoms!\n",
            "\n",
            "In the soil it grew, with love and care,\n",
            "Nourished by sunshine, fresh air to share.\n",
            "Its leaves so green, reaching up so high,\n",
            "A happy carrot, oh my, oh my!\n",
            "\n",
            "With a crunch and a munch, it's oh so tasty,\n",
            "Filled with vitamins, oh so hasty.\n",
            "A happy carrot, a delight to eat,\n",
            "Bringing joy and health, oh what a treat!\n",
            "\n",
            "So let's celebrate this veggie so grand,\n",
            "With a happy carrot in each hand.\n",
            "For in its presence, we surely find,\n",
            "A taste of happiness, one of a kind!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "352ad320",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "352ad320",
        "outputId": "5f1c01e7-d945-44fd-8aa9-693375101b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt_tokens': 37, 'completion_tokens': 164, 'total_tokens': 201}\n"
          ]
        }
      ],
      "source": [
        "print(token_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65372cdd-d869-4768-947a-0173e7f96335",
      "metadata": {
        "id": "65372cdd-d869-4768-947a-0173e7f96335"
      },
      "source": [
        "#### Notes on using the OpenAI API outside of this classroom\n",
        "\n",
        "To install the OpenAI Python library:\n",
        "```\n",
        "!pip install openai\n",
        "```\n",
        "\n",
        "The library needs to be configured with your account's secret key, which is available on the [website](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
        " ```\n",
        " !export OPENAI_API_KEY='sk-...'\n",
        " ```\n",
        "\n",
        "Or, set `openai.api_key` to its value:\n",
        "\n",
        "```\n",
        "import openai\n",
        "openai.api_key = \"sk-...\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8f889c1-f2e4-40a5-bd27-164facb54402",
      "metadata": {
        "id": "d8f889c1-f2e4-40a5-bd27-164facb54402"
      },
      "source": [
        "#### A note about the backslash\n",
        "- In the course, we are using a backslash `\\` to make the text fit on the screen without inserting newline '\\n' characters.\n",
        "- GPT-3 isn't really affected whether you insert newline characters or not.  But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}